{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAN48 Programming for Data Science\n",
    "\n",
    "## Final project\n",
    "\n",
    "As a conclusion to the course I decided to create simple implementation of a feed forward neural network using mainly Numpy and base Python. While there are already exquisite packages that offer these solutions (like Tensorflow and Pytorch), a step by step implementation of a neural network is still valuable for teaching basic programming concepts as well as basic neural network concepts.\n",
    "\n",
    "As a general example I will use the [Kaggle Dogs vs. Cats](https://www.microsoft.com/en-us/download/details.aspx?id=54765) dataset to classify whether a given picture shows a cat or not. As the data set only includes two different options, we can assume the `not cat` option to be the same as `dog`. As mentioned, the intent is to have a general example to expose how the algorithm works, and the intricasies of the programming challenge, in other words, it is *not* my intention to implement a functioning neural network from scratch **and** a good model for classifying cats. \n",
    "\n",
    "## R or Python?\n",
    "\n",
    "This notebook is originally made for Python. One of the requirements for this project was that whatever the choice of application to be developed, it should be done in both Python **and** R. Being that so, this notebook will focus on porting the Python code to R, and therefore, I will not bother in copy pasting the exact same explanations about the data or what a Neural Network is. For that purpose I urge the reader to check the Python notebook with the same name as this one. \n",
    "\n",
    "One practical element of this choice is that at times you will be met with cells starting with `%%writefile`, in those cases one should change the kernel of the notebook (since this command is native to IPyKernel, not R). All else should run smoothly through R.\n",
    "\n",
    "### The structure\n",
    "\n",
    "This project include several files. In this notebook you will find the application related functions, however, many of the base functions used for the calculations are left in a separate file that concentrates all the basic calculation functions. Without those dependencies this notebook will not function as it should. Some basic concepts regarding neural networks will be presented through the notebook, but the focus of this work is exposing the programming challenge behind neural networks.\n",
    "\n",
    "### The data\n",
    "\n",
    "The data set contains 25000 images of dogs and cats, but 59 of them were corrupted or in grayscale and, therefore, dropped. The classes are balanced and the angle, depth, light, and dimensions are not uniform. While originally a Kaggle competition data set, I opted to use the version made available by Microsoft because it did not pre divide the data giving me more freedom to split the sets as I please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(sigmoid) #relu and sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile helper_functions.R\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\" \n",
    "    Takes an input Z and returns max(0,Z) and Z.\n",
    "    Z is \"cached\" so that it can still be used by the backpropagation functions.\n",
    "    \"\"\"\n",
    "    A = np.maximum(0, Z)\n",
    "    assert(A.shape == Z.shape) # Necessary so that A can be used as the new Z in future layers.\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Takes an input Z and returns the sigmoid of it and a cached Z for backpropagation reasons.\n",
    "    \"\"\"\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu_backprop(dA, cache):\n",
    "    \"\"\"\n",
    "    Takes an input dA which is the gradient after activation, and a cached Z.\n",
    "    Returns the gradient of the cost function with respect to Z.\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy = True) #sets the right shape and type to dZ.    \n",
    "    dZ[Z <= 0] = 0 #Need to correct for the cases wehre Z <= 0 since ReLU(.) = max(0, .)\n",
    "    assert(dZ.shape == Z.shape)   \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backprop(dA, cache):\n",
    "    \"\"\"\n",
    "    Takes an input dA which is the gradient after activation, and a cached Z.\n",
    "    Returns the gradient of the cost function with respect to Z.\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z)) #Derivative of sigmoid function.\n",
    "    dZ = dA * s * (1-s)\n",
    "    assert(dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile init_param.py\n",
    "import numpy as np\n",
    "def init_param(layer_dim):\n",
    "    \"\"\"\n",
    "    Takes an input list with the dimensions of each layer in the network and returns a dictionary with parameters W and b.\n",
    "    \"\"\"\n",
    "    parameters={}\n",
    "    L = len(layer_dim) #how many layers there are in the network\n",
    "    for l in range(1, L):\n",
    "        parameters['W'+str(l)]=np.random.randn(layer_dim[l],layer_dim[l-1])*0.01\n",
    "        parameters['b'+str(l)]=np.zeros((layer_dim[l],1))\n",
    "        assert(parameters['W'+str(l)].shape==(layer_dim[l],layer_dim[l-1]))\n",
    "        assert(parameters['b'+str(l)].shape==(layer_dim[l],1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile for_prop.py\n",
    "import numpy as np\n",
    "from helper_functions import sigmoid,relu\n",
    "def for_prop(A,W,b):\n",
    "    \"\"\"\n",
    "    Takes inputs A, W, b, respecitively input data (previous activations), a weights matrix, and a bias vector.\n",
    "    Returns Z (the element which will be used in the activation function), and cached A, W, b and \n",
    "    elements used for backpropagation.\n",
    "    \"\"\"\n",
    "    Z=W.dot(A)+b\n",
    "    assert(Z.shape==(W.shape[0],A.shape[1]))\n",
    "    cache=(A,W,b)\n",
    "    return Z,cache\n",
    "\n",
    "def for_activation(A_prev,W,b,activ):\n",
    "    \"\"\"\n",
    "    Takes A_prev, W, b, and activ, respectively input data (or activations of the previous layer), \n",
    "    a weights matrix, a bias vector, and the activation function be used (either \"sigmoid\" or \"relu\").\n",
    "    Returns the output of the activation and a cached information about the element Z and activation A.\n",
    "    \"\"\"\n",
    "    if activ=='sigmoid':\n",
    "        Z,linear_cache=for_prop(A_prev,W,b)\n",
    "        A,activ_cache=sigmoid(Z)\n",
    "    elif activ=='relu':\n",
    "        Z,linear_cache=for_prop(A_prev,W,b)\n",
    "        A,activ_cache=relu(Z)\n",
    "    assert(A.shape==(W.shape[0],A_prev.shape[1]))\n",
    "    cache=(linear_cache,activ_cache)\n",
    "    return A,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile deep_model.py\n",
    "from for_prop import for_activation\n",
    "def deep_model(X,parameters):\n",
    "    \"\"\"\n",
    "    Takes X and parameters, respectively the data and the initialized parameters from init_param() function.\n",
    "    Returns AV and caches, the activation value at the end of the architecture, and the cached values of every layer.\n",
    "    \"\"\"\n",
    "    caches=[]\n",
    "    A=X\n",
    "    L=len(parameters)//2 #The floor division gives the number of layers in the network\n",
    "    for l in range(1,L):\n",
    "        A_prev=A\n",
    "        A,cache=for_activation(A_prev,parameters['W'+str(l)],parameters['b'+str(l)],activ='relu')\n",
    "        caches.append(cache)\n",
    "    AV,cache=for_activation(A,parameters['W'+str(L)],parameters['b'+str(L)],activ='sigmoid')\n",
    "    caches.append(cache)\n",
    "    assert(AV.shape==(1,X.shape[1]))\n",
    "    return AV,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cost_computation.py\n",
    "import numpy as np\n",
    "def cost_computation(AV,Y):\n",
    "    \"\"\"\n",
    "    Takes AV and Y, the value of the last activation in the NN, and the true labels of the data.\n",
    "    Returns the cross-entropy cost.\n",
    "    \"\"\"\n",
    "    m=Y.shape[1]\n",
    "    cost=-(1/m)*np.sum(np.dot(np.log(AV),Y.T)+np.dot(np.log(1-AV),(1-Y.T)))\n",
    "    cost=np.squeeze(cost) #this makes sure that the values are not expressed as lists inside of lists.\n",
    "    assert(cost.shape == ())\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile back_prop.py\n",
    "import numpy as np\n",
    "from helper_functions import relu_backprop, sigmoid_backprop\n",
    "def back_prop(dZ,cache):\n",
    "    \"\"\"\n",
    "    Takes dZ and cache, the gradient of the cost and the cached values to produce the Z element.\n",
    "    Returns dA_prev, dW, and db, respectively the gradient with respect to the activation, weights, and biases.\n",
    "    \"\"\"\n",
    "    A_prev,W,b=cache\n",
    "    m=A_prev.shape[1]\n",
    "    dW=(1/m)*np.dot(dZ,A_prev.T)\n",
    "    db=(1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev=np.dot(W.T,dZ)\n",
    "    assert(dA_prev.shape==A_prev.shape)\n",
    "    assert(dW.shape==W.shape)\n",
    "    assert(db.shape==b.shape)\n",
    "    return dA_prev,dW,db\n",
    "\n",
    "def back_activ(dA,cache,activ):\n",
    "    \"\"\"\n",
    "    Takes dA (gradient of the activation element), cache (saved values for activation and forward prop),\n",
    "    and the activation type (sigmoid or relu).\n",
    "    Returns dA_prev, dW, and db. Respectively the gradient with respect to the actgivation, weights and biases.\n",
    "    \"\"\"\n",
    "    for_cache,activ_cache=cache\n",
    "    if activ=='relu':\n",
    "        dZ=relu_backprop(dA,activ_cache)\n",
    "        dA_prev,dW,db=back_prop(dZ,for_cache)\n",
    "    elif activ=='sigmoid':\n",
    "        dZ=sigmoid_backprop(dA,activ_cache)\n",
    "        dA_prev,dW,db=back_prop(dZ,for_cache)\n",
    "    return dA_prev,dW,db\n",
    "\n",
    "def deep_model_back(AV,Y,caches):\n",
    "    \"\"\"\n",
    "    Takes AV, Y, and caches. The activation value of the output layer (produced in deep_model()), a vector with the true\n",
    "    labels, and the caches containing all the for_prop() information for each activation function.\n",
    "    Returns a dictionary with gradients dA, dW, and db.\n",
    "    \"\"\"\n",
    "    grads={}\n",
    "    L=len(caches)\n",
    "    m=AV.shape[1]\n",
    "    Y=Y.reshape(AV.shape)\n",
    "    dAL=-(np.divide(Y,AV)-np.divide(1-Y,1-AV)) #the derivative of the cost function I talked about in the preamble of this section\n",
    "    present_cache=caches[L-1]\n",
    "    grads[\"dA\"+str(L-1)],grads[\"dW\"+str(L)],grads[\"db\"+str(L)]=back_activ(dAL,present_cache,activ=\"sigmoid\")\n",
    "    for l in reversed(range(L-1)):\n",
    "        present_cache=caches[l]\n",
    "        dA_prev_temp,dW_temp,db_temp=back_activ(grads['dA'+str(l+1)],present_cache,activ='relu')\n",
    "        grads['dA'+str(l)]=dA_prev_temp\n",
    "        grads['dW'+str(l+1)]=dW_temp\n",
    "        grads['db'+str(l+1)]=db_temp\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile update.py\n",
    "def update(params,grads,learning_rate):\n",
    "    \"\"\"\n",
    "    Takes params, grads, and learning rate, i.e., dictionary with parameters, dictionary with gradients\n",
    "    (from deep_model_back()), and the learning rate of choice.\n",
    "    Returns a dictionary with parameters W and b.\n",
    "    \"\"\"\n",
    "    parameters=params.copy()\n",
    "    L=len(parameters)//2\n",
    "    for l in range(L):\n",
    "        parameters['W'+str(l+1)]=parameters['W'+str(l+1)]-learning_rate*grads['dW'+str(l+1)]\n",
    "        parameters['b'+str(l+1)]=parameters['b'+str(l+1)]-learning_rate*grads['db'+str(l+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_dataset.py\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "def create_dataset(img_folder,IMG_WIDTH=32,IMG_HEIGHT=32):\n",
    "    \"\"\"\n",
    "    Takes the path to the folder containing all images.\n",
    "    Returns a list of images and a list of classes.\n",
    "    \"\"\"\n",
    "    img_data_array=[]\n",
    "    class_name=[]\n",
    "    for dir1 in os.listdir(img_folder):\n",
    "        for file in os.listdir(os.path.join(img_folder,dir1)):\n",
    "            image_path= os.path.join(img_folder,dir1,file)\n",
    "            image= cv2.imread(image_path,cv2.COLOR_BGR2RGB)#For grayscale use cv2.IMREAD_GRAYSCALE\n",
    "            if image is None:\n",
    "                continue\n",
    "            if len(image.shape) ==2: #This effectively removes the grayscale images since they do not have a third dimension\n",
    "                continue\n",
    "            image=cv2.resize(image,(IMG_HEIGHT, IMG_WIDTH),interpolation=cv2.INTER_AREA)\n",
    "            image=np.array(image)\n",
    "            image=image.astype('float32')\n",
    "            image/=255 \n",
    "            img_data_array.append(image)\n",
    "            class_name.append(dir1)\n",
    "    return img_data_array, class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_dataset import create_dataset\n",
    "IMG_WIDTH=64\n",
    "IMG_HEIGHT=64\n",
    "img_folder=r\"C:\\\\Users\\\\wtrindad\\\\source\\\\repos\\\\NN_from_scratch\\\\Step by Step\\\\PetImages\"\n",
    "img_data,class_name=create_dataset(img_folder,IMG_WIDTH,IMG_HEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict={'Cat':1,'Dog':0} #Let's encode our labels\n",
    "class_name=[target_dict[class_name[i]] for i in range(len(class_name))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(img_data,class_name,test_size=.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_flat = x_train.reshape(x_train.shape[0], -1).T\n",
    "print(x_train_flat.shape)\n",
    "\n",
    "x_test_flat = x_test.reshape(x_test.shape[0], -1).T\n",
    "print(x_test_flat.shape)\n",
    "\n",
    "y_train = np.asarray(y_train).reshape(1,-1)\n",
    "print(y_train.shape)\n",
    "\n",
    "y_test = np.asarray(y_test).reshape(1,-1)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [x_train_flat.shape[0],20,7,5,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dense_nn.py\n",
    "import numpy as np\n",
    "from update import update\n",
    "from deep_model import deep_model\n",
    "from init_param import init_param\n",
    "from back_prop import deep_model_back\n",
    "from cost_computation import cost_computation\n",
    "\n",
    "def dense_nn(X,Y,layers_dims,learning_rate=0.0075,num_iterations=5000,print_cost=False):\n",
    "    \"\"\"\n",
    "    Takes X (the data), Y (the labels), a list with the architecture, a given learning rate, number of iterations, and you get to choose whether to print the cost every 100 steps or not.\n",
    "    Returns the parameters of the model which can then be used to make predictions.\n",
    "    \"\"\"\n",
    "    costs=[]\n",
    "    parameters=init_param(layers_dims)\n",
    "    for i in range(0,num_iterations):\n",
    "        AV,caches=deep_model(X,parameters)\n",
    "        cost=cost_computation(AV,Y)\n",
    "        grads=deep_model_back(AV,Y,caches)\n",
    "        parameters=update(parameters,grads,learning_rate)\n",
    "        if print_cost and i%100==0 or i==num_iterations-1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i,np.squeeze(cost)))\n",
    "        if i%100==0 or i==num_iterations:\n",
    "            costs.append(cost)\n",
    "    return parameters,costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dense_nn import dense_nn\n",
    "parameters,costs=dense_nn(x_train_flat,y_train,layer_dims,num_iterations=1,print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters,costs=dense_nn(x_train_flat,y_train,layer_dims,num_iterations=500,print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile predict.py\n",
    "import numpy as np\n",
    "from deep_model import deep_model\n",
    "def predict(X,y,parameters):\n",
    "    \"\"\"\n",
    "    Takes X (the data), y (the labels), and parameters (from the dense_nn() function).\n",
    "    Returns predictions p for the chosen data set X.\n",
    "    \"\"\"\n",
    "    m=X.shape[1]\n",
    "    n=len(parameters)//2\n",
    "    p=np.zeros((1,m))\n",
    "    probs,caches=deep_model(X, parameters)\n",
    "    for i in range(0,probs.shape[1]):\n",
    "        if probs[0,i]>0.5:\n",
    "            p[0,i]=1\n",
    "        else:\n",
    "            p[0,i]=0\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))   \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict import predict\n",
    "predict_train=predict(x_train_flat,y_train,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test=predict(x_test_flat,y_test,parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31565927ecfa8a97530e2f965425b5ecaba213cd38f3e141bd6680a742dfc05e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
